Outlier

occurrence and causes . in the case of normal distribution data , the three sigma rule means that roughly 1 in 22 observations will differ by twice the standard deviation or more from the mean , and 1 in 370 will deviate by three times the standard deviation . in a sample of 1000 observations , the presence of up to five observations deviating from the mean by more than three times the standard deviation is within the range of what can be expected , being less than twice the expected number and hence within 1 standard deviation of the expected number – see poisson distribution – and not indicate an anomaly . if the sample size is only 100 , however , just three such outliers are already reason for concern , being more than 11 times the expected number . in general , if the nature of the population distribution is known a priori , it is possible to test if the number of outliers deviate significantly from what can be expected : for a given cutoff ( so samples fall beyond the cutoff with probability p ) of a given distribution , the number of outliers will follow a binomial distribution with parameter p , which can generally be well-approximated by the poisson distribution with λ pn . thus if one takes a normal distribution with cutoff 3 standard deviations from the mean , p is approximately 0.3 % , and thus for 1000 trials one can approximate the number of samples whose deviation exceeds 3 sigmas by a poisson distribution with λ 3 . causes . outliers can have many anomalous causes . a physical apparatus for taking measurements may have suffered a transient malfunction . there may have been an error in data transmission or transcription . outliers arise due to changes in system behaviour , fraudulent behaviour , human error , instrument error or simply through natural deviations in populations . a sample may have been contaminated with elements from outside the population being examined . alternatively , an outlier could be the result of a flaw in the assumed theory , calling for further investigation by the researcher . additionally , the pathological appearance of outliers of a certain form appears in a variety of datasets , indicating that the causative mechanism for the data might differ at the extreme end ( king effect ) . definitions and detection . there is no rigid mathematical definition of what constitutes an outlier ; determining whether or not an observation is an outlier is ultimately a subjective exercise . there are various methods of outlier detection . some are graphical such as normal probability plots . others are model-based . box plots are a hybrid . model-based methods which are commonly used for identification assume that the data are from a normal distribution , and identify observations which are deemed '' unlikely '' based on mean and standard deviation : chauvenet 's criterion grubbs 's test for outliers dixon 's q test astm e178 standard practice for dealing with outlying observations mahalanobis distance and leverage are often used to detect outliers , especially in the development of linear regression models . subspace and correlation based techniques for high-dimensional numerical data . peirce 's criterion . it is proposed to determine in a series of m observations the limit of error , beyond which all observations involving so great an error may be rejected , provided there are as many as n such observations . the principle upon which it is proposed to solve this problem is , that the proposed observations should be rejected when the probability of the system of errors obtained by retaining them is less than that of the system of errors obtained by their rejection multiplied by the probability of making so many , and no more , abnormal observations . ( quoted in the editorial note on page 516 to peirce ( 1982 edition ) from a manual of astronomy 2:558 by chauvenet . ) benjamin peirce , '' criterion for the rejection of doubtful observations '' , astronomical journal ii 45 ( 1852 ) and errata to the original paper . . noaa pdf eprint ( goes to report p . 200 , pdf 's p . 215 ) . – appendix 21 , according to the editorial note on page 515 . tukey 's fences . other methods flag observations based on measures such as the interquartile range . for example , if q_1 and q_3 are the lower and upper quartiles respectively , then one could define an outlier to be any observation outside the range : : \big q_1 - k ( q_3 - q_1 ) , q_3 + k ( q_3 - q_1 ) \big for some nonnegative constant k . john tukey proposed this test , where k 1.5 indicates an '' outlier '' , and k 3 indicates data that is '' far out '' . . in anomaly detection . in various domains such as , but not limited to , statistics , signal processing , finance , econometrics , manufacturing , networking and data mining , the task of anomaly detection may take other approaches . some of these may be distance-based and density-based such as local outlier factor ( lof ) . some approaches may use the distance to the k-nearest neighbors to label observations as outliers or non-outliers . . modified thompson tau test . the modified thompson tau test is a method used to determine if an outlier exists in a data set . the strength of this method lies in the fact that it takes into account a data set 's standard deviation , average and provides a statistically determined rejection zone ; thus providing an objective method to determine if a data point is an outlier . thompson . r . ( 1985 ) . '' a note on restricted maximum likelihood estimation with an alternative outlier model '' . journal of the royal statistical society . series b ( methodological ) , vol . 47 , no . 1 , pp . 53-55 how it works : first , a data set 's average is determined . next the absolute deviation between each data point and the average are determined . thirdly , a rejection region is determined using the formula : : \text p ( y x , g_j ( t , \alpha ) ) where g_j ( t , \alpha ) is the hypothesis induced by learning algorithm g_j trained on training set with hyperparameters \alpha . instance hardness provides a continuous value for determining if an instance is an outlier instance . working with outliers . the choice of how to deal with an outlier should depend on the cause . some estimators are highly sensitive to outliers , notably estimation of covariance matrices . retention . even when a normal distribution model is appropriate to the data being analyzed , outliers are expected for large sample sizes and should not automatically be discarded if that is the case . the application should use a classification algorithm that is robust to outliers to model data with naturally occurring outlier points . exclusion . deletion of outlier data is a controversial practice frowned upon by many scientists and science instructors ; while mathematical criteria provide an objective and quantitative method for data rejection , they do not make the practice more scientifically or methodologically sound , especially in small sets or where a normal distribution can not be assumed . rejection of outliers is more acceptable in areas of practice where the underlying model of the process being measured and the usual distribution of measurement error are confidently known . an outlier resulting from an instrument reading error may be excluded but it is desirable that the reading is at least verified . the two common approaches to exclude outliers are truncation ( or trimming ) and winsorising . trimming discards the outliers whereas winsorising replaces the outliers with the nearest '' nonsuspect '' data . exclusion can also be a consequence of the measurement process , such as when an experiment is not entirely capable of measuring such extreme values , resulting in censored data . in regression problems , an alternative approach may be to only exclude points which exhibit a large degree of influence on the estimated coefficients , using a measure such as cook 's distance . cook , r . dennis ( feb 1977 ) . '' detection of influential observations in linear regression '' . technometrics ( american statistical association ) 19 ( 1 ) : 15–18 . if a data point ( or points ) is excluded from the data analysis , this should be clearly stated on any subsequent report . non-normal distributions . the possibility should be considered that the underlying distribution of the data is not approximately normal , having '' fat tails '' . for instance , when sampling from a cauchy distribution , weisstein , eric w . cauchy distribution . from mathworld -- a wolfram web resource the sample variance increases with the sample size , the sample mean fails to converge as the sample size increases , and outliers are expected at far larger rates than for a normal distribution . even a slight difference in the fatness of the tails can make a large difference in the expected number of extreme values . set-membership uncertainties . a set estimation considers that the uncertainty corresponding to the ith measurement of an unknown random vector x is represented by a set x i ( instead of a probability density function ) . if no outliers occur , x should belong to the intersection of all x i 's . when outliers occur , this intersection could be empty , and we should relax a small number of the sets x i ( as small as possible ) in order to avoid any inconsistency . this can be done using the notion of q-relaxed intersection . as illustrated by the figure , the q-relaxed intersection corresponds to the set of all x which belong to all sets except q of them . sets x i that do not intersect the q-relaxed intersection could be suspected to be outliers . alternative models . in cases where the cause of the outliers is known , it may be possible to incorporate this effect into the model structure , for example by using a hierarchical bayes model , or a mixture model . roberts , s . and tarassenko , l . : 1995 , a probabilistic resource allocating network for novelty detection . neural computation 6 , 270–284 .